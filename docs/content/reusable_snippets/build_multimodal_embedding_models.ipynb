{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "# Build multimodal embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some embedding models such as [CLIP](https://github.com/openai/CLIP) come in pairs of `model` and `compatible_model`.\n",
    "Otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text>\n",
    "from pinnacledb.ext.sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained sentence transformer model\n",
    "model = SentenceTransformer(\n",
    "    identifier='all-MiniLM-L6-v2',\n",
    "    postprocess=lambda x: x.tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "model.predict_one('some text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Image>\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Import custom modules\n",
    "from pinnacledb.ext.torch import TorchModel, tensor\n",
    "\n",
    "# Define a series of image transformations using torchvision.transforms.Compose\n",
    "t = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # Resize the input image to 224x224 pixels (must same as here)\n",
    "    transforms.CenterCrop((224, 224)),  # Perform a center crop on the resized image\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the tensor with specified mean and standard deviation\n",
    "])\n",
    "\n",
    "# Define a preprocess function that applies the defined transformations to an input image\n",
    "def preprocess(x):\n",
    "    try:\n",
    "        return t(x)\n",
    "    except Exception as e:\n",
    "        # If an exception occurs during preprocessing, issue a warning and return a tensor of zeros\n",
    "        warnings.warn(str(e))\n",
    "        return torch.zeros(3, 224, 224)\n",
    "\n",
    "# Load the pre-trained ResNet-50 model from torchvision\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "# Extract all layers of the ResNet-50 model except the last one\n",
    "modules = list(resnet50.children())[:-1]\n",
    "resnet50 = nn.Sequential(*modules)\n",
    "\n",
    "# Create a TorchModel instance with the ResNet-50 model, preprocessing function, and postprocessing lambda\n",
    "model = TorchModel(\n",
    "    identifier='resnet50',\n",
    "    preprocess=preprocess,\n",
    "    object=resnet50,\n",
    "    postprocess=lambda x: x[:, 0, 0],  # Postprocess by extracting the top-left element of the output tensor\n",
    "    datatype=tensor(dtype='float', shape=(2048,))  # Specify the encoder configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text-Image>\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "import clip\n",
    "from pinnacledb import vector\n",
    "from pinnacledb.ext.torch import TorchModel\n",
    "\n",
    "# Load the CLIP model and obtain the preprocessing function\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device='cpu')\n",
    "\n",
    "# Define a vector with shape (1024,)\n",
    "\n",
    "output_datatpye = vector(shape=(1024,))\n",
    "\n",
    "# Create a TorchModel for text encoding\n",
    "compatible_model = TorchModel(\n",
    "    identifier='clip_text', # Unique identifier for the model\n",
    "    object=model, # CLIP model\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n",
    "    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n",
    "    datatype=output_datatpye,  # Vector encoder with shape (1024,)\n",
    "    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n",
    ")\n",
    "\n",
    "# Create a TorchModel for visual encoding\n",
    "model = TorchModel(\n",
    "    identifier='clip_image',  # Unique identifier for the model\n",
    "    object=model.visual,  # Visual part of the CLIP model    \n",
    "    preprocess=preprocess, # Visual preprocessing using CLIP\n",
    "    postprocess=lambda x: x.tolist(), # Convert the output to a list \n",
    "    datatype=output_datatpye, # Vector encoder with shape (1024,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "compatible_model.predict_one(Image.fromarray(np.ones((256,256,3)).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "model.predict_one('some text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Audio>\n",
    "!pip install librosa\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pinnacledb import ObjectModel\n",
    "from pinnacledb import vector\n",
    "\n",
    "def audio_embedding(audio_file):\n",
    "    # Load the audio file\n",
    "    MAX_SIZE= 10000\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    y = y[:MAX_SIZE]\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=44000, n_mfcc=1)\n",
    "    mfccs =  mfccs.squeeze().tolist()\n",
    "    return mfccs\n",
    "\n",
    "if not get_chunking_datatype:\n",
    "    e =  vector(shape=(1000,))\n",
    "else:\n",
    "    e = get_chunking_datatype(1000)\n",
    "\n",
    "model= ObjectModel(identifier='my-model-audio', object=audio_embedding, datatype=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "import wave\n",
    "import struct\n",
    "\n",
    "sample_rate = 44100 \n",
    "duration = 1 \n",
    "frequency = 440\n",
    "amplitude = 0.5\n",
    "\n",
    "# Generate the sine wave\n",
    "num_samples = int(sample_rate * duration)\n",
    "t = np.linspace(0, duration, num_samples, False)\n",
    "signal = amplitude * np.sin(2 * np.pi * frequency * t)\n",
    "\n",
    "# Open a new WAV file\n",
    "output_file = 'dummy_audio.wav'\n",
    "wav_file = wave.open(output_file, 'w')\n",
    "\n",
    "# Set the parameters for the WAV file\n",
    "nchannels = 1  # Mono audio\n",
    "sampwidth = 2  # Sample width in bytes (2 for 16-bit audio)\n",
    "framerate = sample_rate\n",
    "nframes = num_samples\n",
    "\n",
    "# Set the parameters for the WAV file\n",
    "wav_file.setparams((nchannels, sampwidth, framerate, nframes, 'NONE', 'not compressed'))\n",
    "\n",
    "# Write the audio data to the WAV file\n",
    "for sample in signal:\n",
    "    wav_file.writeframes(struct.pack('h', int(sample * (2 ** 15 - 1))))\n",
    "\n",
    "# Close the WAV file\n",
    "wav_file.close()\n",
    "\n",
    "# Test\n",
    "model.predict_one(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
