<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Image retrieval, captioning and classification with CoCo &mdash; SuperDuperDB  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Full usage" href="../full_usage.html" />
    <link rel="prev" title="Notebook examples" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> SuperDuperDB
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview of SuperDuperDB</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts.html">SuperDuperDB Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cluster.html">Setting up a SuperDuperDB cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models.html">Deep dive into SuperDuperDB models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../semantic_indexes.html">Deep dive into semantic indexes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Model training with SuperDuperDB</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Notebook examples</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Image retrieval, captioning and classification with CoCo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Setting-up-the-data-and-installing-requirements">Setting up the data and installing requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Preparing-the-data-for-injestion">Preparing the data for injestion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Inserting-the-data-and-retrieving-data">Inserting the data and retrieving data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Text-2-image-retrieval-with-pre-trained-model">Text-2-image retrieval with pre-trained model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Bespoke-text-2-image-retriever-using-custom-PyTorch-model">Bespoke text-2-image retriever using custom PyTorch model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Attribute-prediction-using-“imputations”;-transfer-learning,-preparation-using-SpaCy-and-simple-PyTorch-linear-predictor">Attribute prediction using “imputations”; transfer learning, preparation using SpaCy and simple PyTorch linear predictor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Image-captioning-using-recurrent-language-modelling-and-transfer-learning-based-on-CLIP">Image captioning using recurrent language modelling and transfer learning based on CLIP</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Conclusion-and-next-steps">Conclusion and next steps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../full_usage.html">Full usage</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SuperDuperDB</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Notebook examples</a></li>
      <li class="breadcrumb-item active">Image retrieval, captioning and classification with CoCo</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/coco.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Image-retrieval,-captioning-and-classification-with-CoCo">
<h1>Image retrieval, captioning and classification with CoCo<a class="headerlink" href="#Image-retrieval,-captioning-and-classification-with-CoCo" title="Permalink to this heading"></a></h1>
<p>This tutorial uses the <a class="reference external" href="https://cocodataset.org/#home">CoCo dataset “Common objects in Context”</a> to show case some of the key-features of SuperDuperDB. In this example, you’ll learn how to:</p>
<ul class="simple">
<li><p>Prepare data in the best way for SuperDuperDB usage</p></li>
<li><p>Define data types</p></li>
<li><p>Upload and query data to and from the data base</p></li>
<li><p>Define multiple models on the database, including models with dependencies</p></li>
<li><p>Define a searchable semantic index based on existing models</p></li>
<li><p>Train a semantic index from scratch</p></li>
</ul>
<section id="Setting-up-the-data-and-installing-requirements">
<h2>Setting up the data and installing requirements<a class="headerlink" href="#Setting-up-the-data-and-installing-requirements" title="Permalink to this heading"></a></h2>
<p>If you haven’t downloaded the data already, execute the lines of bash below. We’ve tried to keep it clean, and for reasons of efficiency have resized the images using imagemagick.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>mkdir -o data/coco/
<span class="o">!</span>curl http://images.cocodataset.org/annotations/annotations_trainval2014.zip -o data/coco/raw.zip
<span class="o">!</span>unzip data/coco/raw.zip
<span class="o">!</span>mv data/coco/annotations/captions_train2014.json data/coco/
<span class="o">!</span>rm -rf data/coco/annotations
<span class="o">!</span>rm data/coco/raw.zip
<span class="o">!</span>curl http://images.cocodataset.org/zips/train2014.zip -o data/coco/images.zip
<span class="o">!</span>unzip data/coco/images.zip
<span class="o">!</span>rm data/coco/images.zip
<span class="o">!</span>sudo apt install imagemagick
<span class="o">!</span>mogrify -resize 224x data/coco/images/*.jpg
</pre></div>
</div>
</div>
<p>SuperDuperDB uses MongoDB for data storage. If you haven’t done so already, install it using the following lines of bash.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc <span class="p">|</span> sudo apt-key add -
<span class="o">!</span>sudo apt-get install gnupg
<span class="o">!</span>wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc <span class="p">|</span> sudo apt-key add -
<span class="o">!</span><span class="nb">echo</span> <span class="s2">&quot;deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse&quot;</span> <span class="p">|</span> sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list
<span class="o">!</span>sudo apt-get update
<span class="o">!</span>sudo apt-get install -y mongodb-org
</pre></div>
</div>
</div>
<p>In case you haven’t done so already, install the dependencies for this tutorial, including SuperDuperDB, which is a simple pip install.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install pandas
<span class="o">!</span>pip install pillow
<span class="o">!</span>pip install torch
<span class="o">!</span>pip install pinnacledb
</pre></div>
</div>
</div>
</section>
<section id="Preparing-the-data-for-injestion">
<h2>Preparing the data for injestion<a class="headerlink" href="#Preparing-the-data-for-injestion" title="Permalink to this heading"></a></h2>
<p>SuperDuperDB can handle data in any format, including images. The documents in the database are MongoDB <code class="docutils literal notranslate"><span class="pre">bson</span></code> documents, which mix <code class="docutils literal notranslate"><span class="pre">json</span></code> with raw bytes and <code class="docutils literal notranslate"><span class="pre">ObjectId</span></code> objects. SuperDuperDB takes advantage of this by serializing more sophisticated objects to bytes, and reinstantiating the objects in memory, when data is queried.</p>
<p>In order to tell SuperDuperDB what type an object has, one specifies this with a subdocument of the form:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>{
    &quot;_content&quot;: {
        &quot;bytes&quot;: ...,
        &quot;type&quot;: &quot;&lt;my-type&gt;&quot;,
    }
}
</pre></div>
</div>
<p>If however, the content is located on the web or the filesystem, one can specify the URLs directly:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;_content&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="nt">&quot;url&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;url-or-file&gt;&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;my-type&gt;&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>Let’s see this now in action. We reformat the CoCo data, so that each image is associated in one document with all of the captions which describe it, and add the location of the images using the <code class="docutils literal notranslate"><span class="pre">_content</span></code> formalism.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/coco/captions_train2014.json&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">raw</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">raw</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]:</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">raw</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">]}</span>

<span class="k">for</span> <span class="n">im</span> <span class="ow">in</span> <span class="n">raw</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">]:</span>
    <span class="n">raw</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">][</span><span class="n">im</span><span class="p">][</span><span class="s1">&#39;captions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">raw</span><span class="p">[</span><span class="s1">&#39;annotations&#39;</span><span class="p">]:</span>
    <span class="n">raw</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">][</span><span class="n">a</span><span class="p">[</span><span class="s1">&#39;image_id&#39;</span><span class="p">]][</span><span class="s1">&#39;captions&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="s1">&#39;caption&#39;</span><span class="p">])</span>

<span class="n">raw</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">raw</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">im</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">raw</span><span class="p">):</span>
    <span class="c1"># if image is already in memory, then add &#39;bytes&#39;: b&#39;...&#39; instead of &#39;url&#39;: &#39;...&#39;</span>
    <span class="c1"># for content located on the web, use &#39;http://&#39; or &#39;https://&#39; instead of &#39;file://&#39;</span>
    <span class="n">im</span><span class="p">[</span><span class="s1">&#39;img&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;_content&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;file://data/coco/images/</span><span class="si">{</span><span class="n">im</span><span class="p">[</span><span class="s2">&quot;file_name&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;image&#39;</span><span class="p">}</span>
    <span class="p">}</span>
    <span class="n">raw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;captions&#39;</span><span class="p">:</span> <span class="n">im</span><span class="p">[</span><span class="s1">&#39;captions&#39;</span><span class="p">],</span> <span class="s1">&#39;img&#39;</span><span class="p">:</span> <span class="n">im</span><span class="p">[</span><span class="s1">&#39;img&#39;</span><span class="p">]}</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/coco/data.json&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Inserting-the-data-and-retrieving-data">
<h2>Inserting the data and retrieving data<a class="headerlink" href="#Inserting-the-data-and-retrieving-data" title="Permalink to this heading"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;../../&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">pinnacledb.client</span> <span class="kn">import</span> <span class="n">the_client</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">clear_output</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">the_client</span><span class="o">.</span><span class="n">coco_example</span><span class="o">.</span><span class="n">documents</span>
</pre></div>
</div>
</div>
<p>We’ll load the data and add most of it to the database. We’ll hold back some data so that we can see how to update the database later.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/coco/data.json&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">docs</span><span class="o">.</span><span class="n">insert_many</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">]),</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We previously added the type <code class="docutils literal notranslate"><span class="pre">image</span></code> to the <code class="docutils literal notranslate"><span class="pre">_content</span></code> subrecords earlier. So that we can load the data using this type, we need to add this type to the database. You can see in <code class="docutils literal notranslate"><span class="pre">examples/types.py</span></code> how the class encodes and decodes data. Suffice to say at this point, that each type has an <code class="docutils literal notranslate"><span class="pre">encode</span></code> and <code class="docutils literal notranslate"><span class="pre">decode</span></code> method, which convert to and from <code class="docutils literal notranslate"><span class="pre">bytes</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.types</span> <span class="kn">import</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="n">Image</span>

<span class="n">docs</span><span class="o">.</span><span class="n">create_type</span><span class="p">(</span><span class="s1">&#39;float_tensor&#39;</span><span class="p">,</span> <span class="n">FloatTensor</span><span class="p">())</span>
<span class="n">docs</span><span class="o">.</span><span class="n">create_type</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span> <span class="n">Image</span><span class="p">())</span>
</pre></div>
</div>
</div>
</section>
<section id="Text-2-image-retrieval-with-pre-trained-model">
<h2>Text-2-image retrieval with pre-trained model<a class="headerlink" href="#Text-2-image-retrieval-with-pre-trained-model" title="Permalink to this heading"></a></h2>
<p>In the first AI task which we implement for the <code class="docutils literal notranslate"><span class="pre">docs</span></code> collection, we’ll be setting up a model to retrieve relevant images using provided text. For this data, that means the <code class="docutils literal notranslate"><span class="pre">captions</span></code> field being used to retrieve the <code class="docutils literal notranslate"><span class="pre">img</span></code> field. In order to be able to keep an objective record of performance, we can set up an immutable validation dataset from the collection. We use a <strong>splitter</strong> to define how we’d like to test retrieval. This splits the documents into query and retrieved document.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_validation_set</span><span class="p">(</span>
    <span class="s1">&#39;text2image_retrieval&#39;</span><span class="p">,</span>
    <span class="nb">filter</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">splitter</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">({</span><span class="s1">&#39;img&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;img&#39;</span><span class="p">]},</span> <span class="p">{</span><span class="s1">&#39;captions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;captions&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]]}),</span>
    <span class="n">sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can see what the data points in the validation set look like by querying:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="p">[</span><span class="s1">&#39;_validation_sets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">find_one</span><span class="p">({</span><span class="s1">&#39;_validation_set&#39;</span><span class="p">:</span> <span class="s1">&#39;text2image_retrieval&#39;</span><span class="p">})</span>
</pre></div>
</div>
</div>
<p>You can see that the sample “query” is split into the <code class="docutils literal notranslate"><span class="pre">_other</span></code> field. This is important when evaluating semantic indexes.</p>
<p>Now let’s start adding a model to the collection. A nice open source model to test text-2-image retrieval is <a class="reference external" href="https://openai.com/blog/clip/">CLIP</a> which understands images and texts and embeds these in a common vector space.</p>
<p>Note that we are specifying the type of the model output, so that the collection knows how to store the results, as well as “activating” the model with <code class="docutils literal notranslate"><span class="pre">active=True</span></code>. That means, whenever we add data which fall under the <code class="docutils literal notranslate"><span class="pre">filter</span></code>, then these will get processed by the model, and the outputs will be added to the collection documents.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">key</span></code> argument specifies which part of the document the model should act. If <code class="docutils literal notranslate"><span class="pre">key=&quot;_base&quot;</span></code> then the model takes the whole document as input. Since we’ll be encoding documents as images, then we’ll chose <code class="docutils literal notranslate"><span class="pre">key=&quot;img</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.models</span> <span class="kn">import</span> <span class="n">CLIP</span>

<span class="n">docs</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">,</span>
    <span class="nb">object</span><span class="o">=</span><span class="n">CLIP</span><span class="p">(</span><span class="s1">&#39;RN50&#39;</span><span class="p">),</span>
    <span class="nb">filter</span><span class="o">=</span><span class="p">{},</span>
    <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;float_tensor&#39;</span><span class="p">,</span>
    <span class="n">key</span><span class="o">=</span><span class="s1">&#39;img&#39;</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">active</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>We’ll create a companion model which uses the same underlying object as the previous model. That’s specified by adding the name instead of the object in the <code class="docutils literal notranslate"><span class="pre">object</span></code> argument. In this case the model is not <code class="docutils literal notranslate"><span class="pre">active</span></code>, since we’ll only be using it for querying the collection. We don’t need to specify a <code class="docutils literal notranslate"><span class="pre">type</span></code> since that was done in the last step.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;clip_text&#39;</span><span class="p">,</span>
    <span class="nb">object</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">,</span>
    <span class="n">key</span><span class="o">=</span><span class="s1">&#39;captions&#39;</span><span class="p">,</span>
    <span class="n">active</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>We’ll also create a measure which tests how similar to each other two outputs might be. Since CLIP was trained with cosine-similarity we’ll use that here too.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.measures</span> <span class="kn">import</span> <span class="n">css</span>

<span class="n">docs</span><span class="o">.</span><span class="n">create_measure</span><span class="p">(</span><span class="s1">&#39;css&#39;</span><span class="p">,</span> <span class="n">css</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In order to be able to measure performance on the validation set, we’ll add a <strong>metric</strong>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.metrics</span> <span class="kn">import</span> <span class="n">PatK</span>

<span class="n">docs</span><span class="o">.</span><span class="n">create_metric</span><span class="p">(</span><span class="s1">&#39;p_at_10&#39;</span><span class="p">,</span> <span class="n">PatK</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Now we’re ready to go to add a <strong>semantic index</strong>. This is a tuple of models, one of which is activated in order to populate the collection with vectors. The idea is that any of the models in the <strong>semantic index</strong> can be used to query the collection using nearest neighbour lookup based on the <strong>measure</strong> chosen.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.models</span> <span class="kn">import</span> <span class="n">CLIP</span>

<span class="n">docs</span><span class="o">.</span><span class="n">create_semantic_index</span><span class="p">(</span>
    <span class="s1">&#39;clip&#39;</span><span class="p">,</span>
    <span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;clip&#39;</span><span class="p">,</span> <span class="s1">&#39;clip_text&#39;</span><span class="p">],</span>
    <span class="n">measure</span><span class="o">=</span><span class="s1">&#39;css&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;p_at_10&#39;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now the semantic index has been created, we can search through the data using that index.</p>
<p>We can see that we can get nice meaningful retrievals using the CLIP model from short descriptive pieces of text. This is very useful, since the model is now deployed to the database, listening for incoming queries.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">bson</span> <span class="kn">import</span> <span class="n">ObjectId</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>

<span class="n">docs</span><span class="o">.</span><span class="n">semantic_index</span> <span class="o">=</span> <span class="s1">&#39;clip&#39;</span>

<span class="c1"># example using item id directly</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">docs</span><span class="o">.</span><span class="n">find</span><span class="p">({</span><span class="s1">&#39;$like&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;document&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;_id&#39;</span><span class="p">:</span> <span class="n">ObjectId</span><span class="p">(</span><span class="s1">&#39;63d27372745cc274ef3518f2&#39;</span><span class="p">)},</span> <span class="s1">&#39;n&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">}}):</span>
    <span class="n">display</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>

<span class="c1"># or a query which is interpreted by the CLIP model</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">docs</span><span class="o">.</span><span class="n">find</span><span class="p">({</span><span class="s1">&#39;$like&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;document&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;captions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Dog catches a frisbee&#39;</span><span class="p">]},</span> <span class="s1">&#39;n&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}}):</span>
    <span class="n">display</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Let’s now evaluate the quality of this semantic index</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">validate_semantic_index</span><span class="p">(</span><span class="s1">&#39;clip&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;text2image_retrieval&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;p_at_10&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>In the next section of this example, let us train our own model from scratch. The model will be much simpler than the clip model, but will yield faster retrievals. It will be interesting to see how this compares to CLIP, and show-case SuperDuperDB as a framework for easily integrating and benchmarking AI models, in particular for retrieval.</p>
</section>
<section id="Bespoke-text-2-image-retriever-using-custom-PyTorch-model">
<h2>Bespoke text-2-image retriever using custom PyTorch model<a class="headerlink" href="#Bespoke-text-2-image-retriever-using-custom-PyTorch-model" title="Permalink to this heading"></a></h2>
<p>First we will implement a simpler sentence embedding, using a simple word-embedding approach based around Glove. Please look at the model in <code class="docutils literal notranslate"><span class="pre">examples.models.AverageOfGloves</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>curl https://nlp.stanford.edu/data/glove.6B.zip -o data/glove.6B.zip
<span class="o">!</span>unzip data/glove.6B.zip
</pre></div>
</div>
</div>
<p>We may register this model to the collection in the same way we did for the textual part of CLIP:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">examples.models</span> <span class="kn">import</span> <span class="n">AverageOfGloves</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/glove.6B/glove.6B.50d.txt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">float</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>

<span class="n">glove</span> <span class="o">=</span> <span class="n">AverageOfGloves</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span> <span class="n">index</span><span class="p">)</span>

<span class="n">docs</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span>
    <span class="s1">&#39;average_glove&#39;</span><span class="p">,</span>
    <span class="nb">object</span><span class="o">=</span><span class="n">glove</span><span class="p">,</span>
    <span class="n">key</span><span class="o">=</span><span class="s1">&#39;captions&#39;</span><span class="p">,</span>
    <span class="n">active</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>We also need to create a projection layer, which takes the CLIP outputs, and projects them to the same dimensionality as our <code class="docutils literal notranslate"><span class="pre">average_glove</span></code> model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span>
    <span class="s1">&#39;clip_projection&#39;</span><span class="p">,</span>
    <span class="nb">object</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="n">active</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">key</span><span class="o">=</span><span class="s1">&#39;img&#39;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;float_tensor&#39;</span><span class="p">,</span>
    <span class="n">features</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;img&#39;</span><span class="p">:</span> <span class="s1">&#39;clip&#39;</span><span class="p">},</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s also create a loss function, in order to be able to perform the learning task:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.losses</span> <span class="kn">import</span> <span class="n">ranking_loss</span>

<span class="n">docs</span><span class="o">.</span><span class="n">create_loss</span><span class="p">(</span><span class="s1">&#39;ranking_loss&#39;</span><span class="p">,</span> <span class="n">ranking_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>A semantic index training requires:</p>
<ul class="simple">
<li><p>1 or more models</p></li>
<li><p>A measure function to measure similarity between model outputs</p></li>
<li><p>A loss function</p></li>
<li><p>One or more validation sets</p></li>
<li><p>One or more metrics to measure performance</p></li>
</ul>
<p>We now have all of these things ready and registered with the database, so we can start the training:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_semantic_index</span><span class="p">(</span>
    <span class="s1">&#39;simple_image_search&#39;</span><span class="p">,</span>
    <span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;clip_projection&#39;</span><span class="p">,</span> <span class="s1">&#39;average_glove&#39;</span><span class="p">],</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;ranking_loss&#39;</span><span class="p">,</span>
    <span class="nb">filter</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">projection</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;image&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;_like&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;p_at_10&#39;</span><span class="p">],</span>
    <span class="n">measure</span><span class="o">=</span><span class="s1">&#39;css&#39;</span><span class="p">,</span>
    <span class="n">validation_sets</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text2image_retrieval&#39;</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">n_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">log_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">validation_interval</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">no_improve_then_stop</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">n_iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
    <span class="n">use_grads</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;clip_projection&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;average_glove&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>We now can see that we’ve set and trained our own semantic index. Let’s take a look at the results of the training:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">info</span> <span class="o">=</span> <span class="n">docs</span><span class="p">[</span><span class="s1">&#39;_semantic_indexes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">find_one</span><span class="p">({</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;simple_image_search&#39;</span><span class="p">})</span>
</pre></div>
</div>
</div>
<p>We can visualize the improvement of metrics during training using standard functionality from the scientific Python ecosystem. No need for Tensorboards or special visualization interfaces!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;metric_values&#39;</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;metric_values&#39;</span><span class="p">][</span><span class="n">k</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;metric_values&#39;</span><span class="p">][</span><span class="n">k</span><span class="p">])</span>
        <span class="k">continue</span>
    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;metric_values&#39;</span><span class="p">][</span><span class="n">k</span><span class="p">]:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;metric_values&#39;</span><span class="p">][</span><span class="n">k</span><span class="p">][</span><span class="n">result</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>The same can be done for the progression of weights during training:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;weights&#39;</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;weights&#39;</span><span class="p">][</span><span class="n">parameter</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">refresh_model</span><span class="p">(</span><span class="s1">&#39;clip_projection&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We see that our model provides quick retrievals using its simpler architecture, and we succeeded in doing this with a very small resource footprint:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>

<span class="n">docs</span><span class="o">.</span><span class="n">semantic_index</span> <span class="o">=</span> <span class="s1">&#39;simple_image_search&#39;</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">docs</span><span class="o">.</span><span class="n">find</span><span class="p">({</span><span class="s1">&#39;$like&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;document&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;captions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Dog catches frisbee&#39;</span><span class="p">]},</span> <span class="s1">&#39;n&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}}):</span>
    <span class="n">display</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Let’s see how well our model has done:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">validate_semantic_index</span><span class="p">(</span><span class="s1">&#39;simple_image_search&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;text2image_retrieval&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;p_at_10&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</section>
<section id="Attribute-prediction-using-“imputations”;-transfer-learning,-preparation-using-SpaCy-and-simple-PyTorch-linear-predictor">
<h2>Attribute prediction using “imputations”; transfer learning, preparation using SpaCy and simple PyTorch linear predictor<a class="headerlink" href="#Attribute-prediction-using-“imputations”;-transfer-learning,-preparation-using-SpaCy-and-simple-PyTorch-linear-predictor" title="Permalink to this heading"></a></h2>
<p>Now we’re going to train a different type of model, using a meta-class of machine learning problems we call “imputation”. The basic idea is to generate tags, by using as training data the noun words extracted from the sentences attached to each image. For this we’ll need one model to prepare the data, by extracting these noun words, and another model to predict the tags. Imputations aren’t restricted to this however. We can handle any machine learning problem which involves predicting one thing
from another thing. This subsumes classification, attribute prediction, generative adversarial learning, language modelling and more.</p>
<p>For the noun extraction, we’ll use an external library <code class="docutils literal notranslate"><span class="pre">spacy</span></code>. You can see this in the source code <code class="docutils literal notranslate"><span class="pre">examples.models.NounWords</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.models</span> <span class="kn">import</span> <span class="n">NounWords</span>
<span class="n">docs</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">&#39;noun_words&#39;</span><span class="p">,</span> <span class="n">NounWords</span><span class="p">(),</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;captions&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We’ll also need a simple validation set:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_validation_set</span><span class="p">(</span><span class="s1">&#39;attribute_prediction&#39;</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">250</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s prepare the tag set we’d like to train on in the next few cells:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">tqdm</span>
<span class="n">all_nouns</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">docs</span><span class="o">.</span><span class="n">find</span><span class="p">({</span><span class="s1">&#39;_fold&#39;</span><span class="p">:</span> <span class="s1">&#39;train&#39;</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;_outputs.captions.noun_words&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}),</span>
                   <span class="n">total</span><span class="o">=</span><span class="n">docs</span><span class="o">.</span><span class="n">count_documents</span><span class="p">({})):</span>
    <span class="n">all_nouns</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;_outputs&#39;</span><span class="p">][</span><span class="s1">&#39;captions&#39;</span><span class="p">][</span><span class="s1">&#39;noun_words&#39;</span><span class="p">])</span>

<span class="n">counts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">all_nouns</span><span class="p">))</span>
<span class="n">all_nouns</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">counts</span> <span class="k">if</span> <span class="n">counts</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">]</span>
<span class="n">total</span> <span class="o">=</span> <span class="n">docs</span><span class="o">.</span><span class="n">count_documents</span><span class="p">({})</span>
<span class="n">pos_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">counts</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">/</span> <span class="n">total</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">all_nouns</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>Now we’ve got our tag set ready, we can create the items necessary to train the model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.models</span> <span class="kn">import</span> <span class="n">FewHot</span><span class="p">,</span> <span class="n">TopK</span>
<span class="kn">from</span> <span class="nn">examples.metrics</span> <span class="kn">import</span> <span class="n">jacquard_index</span>

<span class="n">docs</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">&#39;nouns_to_few_hot&#39;</span><span class="p">,</span> <span class="n">FewHot</span><span class="p">(</span><span class="n">all_nouns</span><span class="p">))</span>
<span class="n">docs</span><span class="o">.</span><span class="n">create_postprocessor</span><span class="p">(</span><span class="s1">&#39;top_5&#39;</span><span class="p">,</span> <span class="n">TopK</span><span class="p">(</span><span class="n">all_nouns</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">docs</span><span class="o">.</span><span class="n">create_forward</span><span class="p">(</span><span class="s1">&#39;attribute_predictor&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_nouns</span><span class="p">)))</span>
<span class="n">docs</span><span class="o">.</span><span class="n">create_loss</span><span class="p">(</span><span class="s1">&#39;nouns_loss&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">pos_weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pos_weights</span><span class="p">)))</span>
<span class="n">docs</span><span class="o">.</span><span class="n">create_metric</span><span class="p">(</span><span class="s1">&#39;jacquard_index&#39;</span><span class="p">,</span> <span class="n">jacquard_index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We demonstrate a different way of creating a model. Instead of using a class with <code class="docutils literal notranslate"><span class="pre">preprocess</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code> and <code class="docutils literal notranslate"><span class="pre">postprocess</span></code> methods, we supply these separately. Sometimes this is advantageous when it comes to sharing aspects between models.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">&#39;attribute_predictor&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="s1">&#39;attribute_predictor&#39;</span><span class="p">,</span> <span class="n">postprocessor</span><span class="o">=</span><span class="s1">&#39;top_5&#39;</span><span class="p">,</span>
                  <span class="n">key</span><span class="o">=</span><span class="s1">&#39;img&#39;</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;img&#39;</span><span class="p">:</span> <span class="s1">&#39;clip&#39;</span><span class="p">})</span>
</pre></div>
</div>
</div>
<p>Let’s test the model, using the <code class="docutils literal notranslate"><span class="pre">apply_model</span></code> method:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="s1">&#39;attribute_predictor&#39;</span><span class="p">,</span> <span class="n">docs</span><span class="o">.</span><span class="n">find_one</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>Now let’s train the model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_imputation</span><span class="p">(</span>
    <span class="s1">&#39;noun_prediction&#39;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s1">&#39;attribute_predictor&#39;</span><span class="p">,</span>
    <span class="n">target</span><span class="o">=</span><span class="s1">&#39;nouns_to_few_hot&#39;</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;nouns_loss&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;jacquard_index&#39;</span><span class="p">],</span>
    <span class="n">validation_sets</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;attribute_prediction&#39;</span><span class="p">],</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">validation_interval</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">n_iterations</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can view the results of learning (metrics, loss etc.) by looking in the <code class="docutils literal notranslate"><span class="pre">_imputations</span></code> subcollection:</p>
</section>
<section id="Image-captioning-using-recurrent-language-modelling-and-transfer-learning-based-on-CLIP">
<h2>Image captioning using recurrent language modelling and transfer learning based on CLIP<a class="headerlink" href="#Image-captioning-using-recurrent-language-modelling-and-transfer-learning-based-on-CLIP" title="Permalink to this heading"></a></h2>
<p>In the final modelling part of this tutorial, we show that a radically different type of model can be created but which also leverages the <code class="docutils literal notranslate"><span class="pre">create_imputation</span></code> functionality. This is possible, because we utilize a different loss, target, and also use a model which has a different inference <code class="docutils literal notranslate"><span class="pre">forward</span></code> pass than training <code class="docutils literal notranslate"><span class="pre">forward</span></code> pass. This is a very common occurrence, in AI, especially when doing, for example, autoregressive training.</p>
<p>The model we create, will be input an image, and will write out a sentence describing that image in unconstrained English. This task is known as “captioning” in AI speak.</p>
<p>This model will leverage a fixed vocabulary of “allowed” words. Let us first create this quickly in the following cell:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tqdm</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="n">all_captions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">docs</span><span class="o">.</span><span class="n">count_documents</span><span class="p">({</span><span class="s1">&#39;_fold&#39;</span><span class="p">:</span> <span class="s1">&#39;train&#39;</span><span class="p">})</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm_notebook</span><span class="p">(</span><span class="n">docs</span><span class="o">.</span><span class="n">find</span><span class="p">({</span><span class="s1">&#39;_fold&#39;</span><span class="p">:</span> <span class="s1">&#39;train&#39;</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;captions&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;_id&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}),</span> <span class="n">total</span><span class="o">=</span><span class="n">n</span><span class="p">):</span>
    <span class="n">all_captions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;captions&#39;</span><span class="p">])</span>

<span class="n">all_captions</span> <span class="o">=</span> <span class="p">[</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;[^a-z ]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_captions</span><span class="p">]</span>
<span class="n">words</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">all_captions</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">counts</span> <span class="k">if</span> <span class="n">counts</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">w</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Now we can create the model - it utilizes a “tokenizer” for preprocessing the captioning data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.models</span> <span class="kn">import</span> <span class="n">ConditionalLM</span><span class="p">,</span> <span class="n">SimpleTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SimpleTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">ConditionalLM</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let us know create the required models necessary for training this model. One of the models is fairly trivial, only used to create the prediction target for the learning task:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">&#39;conditional_lm&#39;</span><span class="p">,</span> <span class="nb">object</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">active</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;img&#39;</span><span class="p">:</span> <span class="s1">&#39;clip&#39;</span><span class="p">},</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;_base&#39;</span><span class="p">)</span>
<span class="n">docs</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">&#39;captioning_tokenizer&#39;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;caption&#39;</span><span class="p">,</span> <span class="n">active</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We’ll use a standard autoregressive loss, of the sort used as a matter of course in language modelling tasks.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.losses</span> <span class="kn">import</span> <span class="n">auto_regressive_loss</span>
<span class="n">docs</span><span class="o">.</span><span class="n">create_loss</span><span class="p">(</span><span class="s1">&#39;autoregressive_loss&#39;</span><span class="p">,</span> <span class="n">auto_regressive_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Since each record in the database has several captions per image, we’ll need to use a so-called “splitter”, to align the prediction model and prediction target during training. You can see that the splitter randomly chooses one of the captions to train on for an iteration:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">examples.splitters</span> <span class="kn">import</span> <span class="n">captioning_splitter</span>

<span class="n">docs</span><span class="o">.</span><span class="n">create_splitter</span><span class="p">(</span><span class="s1">&#39;captioning_splitter&#39;</span><span class="p">,</span> <span class="n">captioning_splitter</span><span class="p">)</span>

<span class="n">captioning_splitter</span><span class="p">(</span><span class="n">docs</span><span class="o">.</span><span class="n">find_one</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>Since we have this new splitter, we need to create a new validation data set</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_validation_set</span><span class="p">(</span><span class="s1">&#39;captioning&#39;</span><span class="p">,</span> <span class="n">splitter</span><span class="o">=</span><span class="n">docs</span><span class="o">.</span><span class="n">splitters</span><span class="p">[</span><span class="s1">&#39;captioning_splitter&#39;</span><span class="p">],</span>
                           <span class="n">sample_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now we’re ready to start the training:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">create_imputation</span><span class="p">(</span>
    <span class="s1">&#39;image_captioner&#39;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s1">&#39;conditional_lm&#39;</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;autoregressive_loss&#39;</span><span class="p">,</span>
    <span class="n">target</span><span class="o">=</span><span class="s1">&#39;captioning_tokenizer&#39;</span><span class="p">,</span>
    <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;captioning_splitter&#39;</span><span class="p">,</span>
    <span class="n">validation_sets</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;captioning&#39;</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s test the model on a sample data point:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_docs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">docs</span><span class="o">.</span><span class="n">find</span><span class="p">()</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
<span class="n">images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">docs</span><span class="o">.</span><span class="n">find</span><span class="p">({},</span> <span class="p">{</span><span class="s1">&#39;img&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">docs</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="s1">&#39;conditional_lm&#39;</span><span class="p">,</span> <span class="n">test_docs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
    <span class="n">display</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 18.98it/s]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_1.png" src="../_images/examples_coco_93_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a dining room with a table and chairs in front of a &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_3.png" src="../_images/examples_coco_93_3.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a person is cutting into a cake with a &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_5.png" src="../_images/examples_coco_93_5.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a man standing next to a red and white &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_7.png" src="../_images/examples_coco_93_7.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a kitchen with a &lt;unk&gt; &lt;unk&gt; and other &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_9.png" src="../_images/examples_coco_93_9.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a man and a woman are standing in a kitchen &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_11.png" src="../_images/examples_coco_93_11.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a man wearing a suit and tie in front of a &lt;unk&gt; &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_13.png" src="../_images/examples_coco_93_13.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a wooden table with a &lt;unk&gt; &lt;unk&gt; and a &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_15.png" src="../_images/examples_coco_93_15.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a group of people walking down a street with a &lt;unk&gt; &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_17.png" src="../_images/examples_coco_93_17.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a bicycle is parked next to a bicycle &lt;unk&gt; and &lt;unk&gt; &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_19.png" src="../_images/examples_coco_93_19.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a man is working on a kitchen counter &lt;unk&gt; into a &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_21.png" src="../_images/examples_coco_93_21.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a man in a white kitchen with a white &lt;unk&gt; &lt;unk&gt; and
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_23.png" src="../_images/examples_coco_93_23.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a man is preparing food in a &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_25.png" src="../_images/examples_coco_93_25.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a kitchen with a &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; and
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_27.png" src="../_images/examples_coco_93_27.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a kitchen with a &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_29.png" src="../_images/examples_coco_93_29.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a man is preparing food in a &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_31.png" src="../_images/examples_coco_93_31.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a man sitting on a laptop with a &lt;unk&gt; &lt;unk&gt; on his
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_33.png" src="../_images/examples_coco_93_33.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a group of people are standing around a &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_35.png" src="../_images/examples_coco_93_35.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a man cutting a pizza on a &lt;unk&gt; &lt;unk&gt; in a &lt;unk&gt;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_37.png" src="../_images/examples_coco_93_37.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a large display of a &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt; and &lt;unk&gt; in
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_coco_93_39.png" src="../_images/examples_coco_93_39.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a room with a &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;
</pre></div></div>
</div>
<p>Now we have trained and evaluated several models of various types. This includes multiple interacting models with mutual dependencies. In the case of our own efficient semantic search, and also the attribute predictor, these models are downstream of the image clip model, in the sense that at inference time, clip must be present in order to be able to execute these models. In the case of attribute prediction, the training task was downstream from the spacy pipeline for part-of-speech tagging;
these tags were used to produce targets for training. However at run-time, the spacy pipeline won’t be necessary.</p>
<p>The models which we’ve added and trained are now ready to go, and when new data is added or updated to the collection, they will automatically process this data, and insert the model outputs into the collection documents.</p>
<p>Here is the complete set of models which exist in the collection:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">list_models</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;attribute_predictor&#39;,
 &#39;average_glove&#39;,
 &#39;captioning_tokenizer&#39;,
 &#39;clip&#39;,
 &#39;clip_projection&#39;,
 &#39;clip_text&#39;,
 &#39;conditional_lm&#39;,
 &#39;noun_words&#39;,
 &#39;nouns_to_few_hot&#39;]
</pre></div></div>
</div>
<p>Not all of these respond to incoming data, for that we need to specify the <code class="docutils literal notranslate"><span class="pre">active</span></code> argument:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">list_models</span><span class="p">(</span><span class="n">active</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;attribute_predictor&#39;, &#39;clip&#39;, &#39;clip_projection&#39;, &#39;noun_words&#39;]
</pre></div></div>
</div>
<p>We can see that these models have processed all documents and their outputs saved:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span><span class="o">.</span><span class="n">find_one</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;_id&#39;: ObjectId(&#39;63d27372745cc274ef3518f2&#39;),
 &#39;captions&#39;: [&#39;A restaurant has modern wooden tables and chairs.&#39;,
  &#39;A long restaurant table with rattan rounded back chairs.&#39;,
  &#39;a long table with a plant on top of it surrounded with wooden chairs &#39;,
  &#39;A long table with a flower arrangement in the middle for meetings&#39;,
  &#39;A table is adorned with wooden chairs with blue accents.&#39;],
 &#39;img&#39;: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x168&gt;,
 &#39;_fold&#39;: &#39;train&#39;,
 &#39;_outputs&#39;: {&#39;img&#39;: {&#39;clip&#39;: tensor([ 0.0203,  0.0837,  0.0035,  ..., -0.0788,  0.0529, -0.1146]),
   &#39;clip_projection&#39;: tensor([ 0.0276,  0.0818, -0.0482, -0.0414,  0.0886, -0.0940,  0.0234, -0.0745,
           -0.1247, -0.0518, -0.0856, -0.0426,  0.0444,  0.0320,  0.0247,  0.0495,
           -0.0079, -0.0169,  0.1407, -0.1423,  0.1019, -0.0112, -0.0730, -0.0060,
           -0.0891,  0.1499, -0.0039,  0.0558, -0.0639,  0.0555,  0.0299, -0.0312,
            0.0298, -0.0540,  0.0155,  0.0800,  0.0311,  0.0915,  0.0415,  0.0481,
            0.0284,  0.0069,  0.0161,  0.0645, -0.0368,  0.1177, -0.0357, -0.0539,
           -0.0467, -0.0735]),
   &#39;attribute_predictor&#39;: [&#39;dishwasher&#39;,
    &#39;mask&#39;,
    &#39;containers&#39;,
    &#39;car&#39;,
    &#39;stall&#39;]},
  &#39;_base&#39;: {},
  &#39;captions&#39;: {&#39;noun_words&#39;: [&#39;accents&#39;,
    &#39;arrangement&#39;,
    &#39;chairs&#39;,
    &#39;flower&#39;,
    &#39;meetings&#39;,
    &#39;middle&#39;,
    &#39;plant&#39;,
    &#39;restaurant&#39;,
    &#39;table&#39;,
    &#39;tables&#39;,
    &#39;top&#39;]}}}
</pre></div></div>
</div>
</section>
<section id="Conclusion-and-next-steps">
<h2>Conclusion and next steps<a class="headerlink" href="#Conclusion-and-next-steps" title="Permalink to this heading"></a></h2>
<p>In this tutorial we showed how to deploy interdependent open source, and user supplied PyTorch models on a collection of records, containing image and text. SuperDuperDB can be used on arbitrary data types, and allows users to supply arbitrary PyTorch models. These may additionally be trained or fine-tuned on the data at hand.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Notebook examples" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../full_usage.html" class="btn btn-neutral float-right" title="Full usage" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Duncan Blythe duncan@pinnacledb.com.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>