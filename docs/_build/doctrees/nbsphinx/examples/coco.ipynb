{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238f94f6",
   "metadata": {},
   "source": [
    "# Image retrieval, captioning and classification with CoCo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38bc025",
   "metadata": {},
   "source": [
    "This tutorial uses the [CoCo dataset \"Common objects in Context\"](https://cocodataset.org/#home) to show case some of the key-features of SuperDuperDB. In this example, you'll learn how to:\n",
    "\n",
    "- Prepare data in the best way for SuperDuperDB usage\n",
    "- Define data types\n",
    "- Upload and query data to and from the data base\n",
    "- Define multiple models on the database, including models with dependencies\n",
    "- Define a searchable semantic index based on existing models\n",
    "- Train a semantic index from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8792e",
   "metadata": {},
   "source": [
    "## Setting up the data and installing requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66577c0c",
   "metadata": {},
   "source": [
    "If you haven't downloaded the data already, execute the lines of bash below. We've tried to keep it clean,\n",
    "and for reasons of efficiency have resized the images using imagemagick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6232ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://images.cocodataset.org/annotations/annotations_trainval2014.zip -o raw.zip\n",
    "!unzip raw.zip\n",
    "!curl http://images.cocodataset.org/zips/train2014.zip -o images.zip\n",
    "!unzip -q images.zip\n",
    "!sudo apt install -y imagemagick\n",
    "!mogrify -resize 224x 'train2014/*.jpg'\n",
    "!mkdir -p data/coco\n",
    "!mv train2014 data/coco/images\n",
    "!mv annotations/* data/coco\n",
    "!rm raw.zip\n",
    "!rm images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455a1c8",
   "metadata": {},
   "source": [
    "SuperDuperDB uses MongoDB for data storage. If you haven't done so already, install it using the following lines of bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ec394",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo apt-key add -\n",
    "!sudo apt-get install gnupg\n",
    "!wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo apt-key add -\n",
    "!echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y mongodb-org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6dc87",
   "metadata": {},
   "source": [
    "In case you haven't done so already, install the dependencies for this tutorial, including SuperDuperDB,\n",
    "which is a simple pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pillow\n",
    "!pip install torch\n",
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767eadeb",
   "metadata": {},
   "source": [
    "## Preparing the data for injestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1e6e1",
   "metadata": {},
   "source": [
    "SuperDuperDB can handle data in any format, including images. The documents in the database are MongoDB `bson` documents, which mix `json` with raw bytes and `ObjectId` objects. SuperDuperDB takes advantage of this by \n",
    "serializing more sophisticated objects to bytes, and reinstantiating the objects in memory, when data is queried.\n",
    "\n",
    "In order to tell SuperDuperDB what type an object has, one specifies this with a subdocument of the form:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"_content\": {\n",
    "        \"bytes\": ...,\n",
    "        \"type\": \"<my-type>\",\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "If however, the content is located on the web or the filesystem, one can specify the URLs directly:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"_content\": {\n",
    "        \"url\": \"<url-or-file>\",\n",
    "        \"type\": \"<my-type>\",\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Let's see this now in action. We reformat the CoCo data, so that each image is associated in one document with all of the captions which describe it, and add the location of the images using the `_content` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a5dc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"captions\": [\n",
      "      \"A restaurant has modern wooden tables and chairs.\",\n",
      "      \"A long restaurant table with rattan rounded back chairs.\",\n",
      "      \"a long table with a plant on top of it surrounded with wooden chairs \",\n",
      "      \"A long table with a flower arrangement in the middle for meetings\",\n",
      "      \"A table is adorned with wooden chairs with blue accents.\"\n",
      "    ],\n",
      "    \"img\": {\n",
      "      \"_content\": {\n",
      "        \"url\": \"file://data/coco/images/COCO_train2014_000000057870.jpg\",\n",
      "        \"type\": \"image\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"captions\": [\n",
      "      \"A man preparing desserts in a kitchen covered in frosting.\",\n",
      "      \"A chef is preparing and decorating many small pastries.\",\n",
      "      \"A baker prepares various types of baked goods.\",\n",
      "      \"a close up of a person grabbing a pastry in a container\",\n",
      "      \"Close up of a hand touching various pastries.\"\n",
      "    ],\n",
      "    \"img\": {\n",
      "      \"_content\": {\n",
      "        \"url\": \"file://data/coco/images/COCO_train2014_000000384029.jpg\",\n",
      "        \"type\": \"image\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data/coco/captions_train2014.json') as f:\n",
    "    raw = json.load(f)\n",
    "    \n",
    "raw['images'] = {x['id']: x for x in raw['images']}\n",
    "\n",
    "for im in raw['images']:\n",
    "    raw['images'][im]['captions'] = []\n",
    "    \n",
    "for a in raw['annotations']:\n",
    "    raw['images'][a['image_id']]['captions'].append(a['caption'])\n",
    "\n",
    "raw = list(raw['images'].values())\n",
    "\n",
    "for i, im in enumerate(raw):\n",
    "    # if image is already in memory, then add 'bytes': b'...' instead of 'url': '...'\n",
    "    # for content located on the web, use 'http://' or 'https://' instead of 'file://'\n",
    "    im['img'] = {\n",
    "        '_content': {'url': f'file://data/coco/images/{im[\"file_name\"]}', 'type': 'image'}\n",
    "    }\n",
    "    raw[i] = {'captions': im['captions'], 'img': im['img']}\n",
    "    \n",
    "print(json.dumps(raw[:2], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff7bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/coco/data.json', 'w') as f:\n",
    "    json.dump(raw, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe19d9",
   "metadata": {},
   "source": [
    "## Inserting the data and retrieving data\n",
    "\n",
    "Importing a collection with SuperDuperDB is exactly like importing a collection with PyMongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a45f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from pinnacledb.client import the_client\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "\n",
    "docs = the_client.coco.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b03ae",
   "metadata": {},
   "source": [
    "We'll load the data and add most of it to the database. We'll hold back some data so that we can see how to update \n",
    "the database later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9bf022c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading content from retrieved urls\n",
      "found 81783 urls\n",
      "number of workers 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 81783/81783 [04:53<00:00, 278.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x118cfb5e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/coco/data.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "docs.insert_many(data[:-1000], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433f7c0",
   "metadata": {},
   "source": [
    "We previously added the type `image` to the `_content` subrecords earlier. This gives SuperDuperDB the chance to interpret the raw data within the `_content.bytes` field.\n",
    "\n",
    "\n",
    "So that we can load the data using this type, we need to create this type and add it to the database.\n",
    "Each type is a class with an `encode` and `decode` method, which convert to and from `bytes`.\n",
    "\n",
    "We import the types from the `examples` directory on GitHub. However, in this notebook we include the code snippets for completeness:\n",
    "\n",
    "In `examples.types`:\n",
    "\n",
    "```python\n",
    "import io\n",
    "import numpy\n",
    "import PIL.Image\n",
    "import torch\n",
    "\n",
    "\n",
    "class Image:\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(x):\n",
    "        buffer = io.BytesIO()\n",
    "        x.save(buffer, format='png')\n",
    "        return buffer.getvalue()\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(bytes_):\n",
    "        return PIL.Image.open(io.BytesIO(bytes_))\n",
    "\n",
    "\n",
    "class FloatTensor:\n",
    "    types = (torch.FloatTensor, torch.Tensor)\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(x):\n",
    "        x = x.numpy()\n",
    "        assert x.dtype == numpy.float32\n",
    "        return memoryview(x).tobytes()\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(bytes_):\n",
    "        array = numpy.frombuffer(bytes_, dtype=numpy.float32)\n",
    "        return torch.from_numpy(array).type(torch.float)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2728cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.types import FloatTensor, Image\n",
    "\n",
    "docs.create_type('float_tensor', FloatTensor())\n",
    "docs.create_type('image', Image())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f537aa",
   "metadata": {},
   "source": [
    "## Text-2-image retrieval with pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70298808",
   "metadata": {},
   "source": [
    "In the first AI task which we implement for this collection of data, we'll be setting up a model to retrieve relevant images using provided text. We'll use the data from the `captions` field to retrieve the `img` field. In order to be able to keep an objective record of performance, it's necessary up to set up a validation dataset from the collection. We use a **splitter** to define how we'd like to test retrieval. This splits the documents into query and retrieved document.\n",
    "\n",
    "You may have noticed that the records are already randomly split into \"train\" and \"valid\" folds during data insertion. The goal of creating a validation set, is to fix the data for the model evaluation once and for all. Otherwise, when new data come in, the documents with `\"_fold\": \"valid\"` may have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06d956c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('63f36c946bb7bf3d196c86ce'),\n",
       " 'captions': ['a table topped with bowls of food and a glass of water.',\n",
       "  'Dishes of prepared food laid out on a table with empty glasses and plates',\n",
       "  'Several bowls of food next to a stack of plates.',\n",
       "  'Prepared Asian foods sit on a table with dinner plates and glasses.',\n",
       "  'A black table holds white bowls and plates and different foods as water glasses also sit on the table.'],\n",
       " 'img': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x149>,\n",
       " '_fold': 'train'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = docs.find(raw=False)[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8565389",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1013.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading content from retrieved urls\n",
      "found 0 urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from examples.splitters import retrieval_splitter\n",
    "\n",
    "docs.create_splitter('retrieval_splitter', retrieval_splitter)\n",
    "docs.create_validation_set(\n",
    "    'text2image_retrieval', \n",
    "    filter={},\n",
    "    splitter=lambda x: ({'img': x['img']}, {'captions': [x['captions'][0]]}),\n",
    "    sample_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c91f7",
   "metadata": {},
   "source": [
    "We can see what the data points in the validation set look like by querying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c171e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x1206cc790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs['_validation_sets'].delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2550b",
   "metadata": {},
   "source": [
    "You can see that the sample \"query\" is split into the `_other` field. This is important when evaluating semantic indexes.\n",
    "\n",
    "Now let's start adding a model to the collection.\n",
    "A nice open source model to test text-2-image retrieval is [CLIP](https://openai.com/blog/clip/) which understands images and texts and embeds these in a common vector space.\n",
    "\n",
    "Note that we are specifying the type of the model output, so that the collection knows how to store the results, as well as \"activating\" the model with `active=True`. That means, whenever we add data which fall under the `filter`, then these will get processed by the model, and the outputs will be added to the collection documents.\n",
    "\n",
    "The `key` argument specifies which part of the document the model should act. If `key=\"_base\"` then the model takes the whole document as input. Since we'll be encoding documents as images, then we'll chose `key=\"img`.\n",
    "\n",
    "We import the clip model from the `examples` directory on GitHub. However, for completeness, we quote the code here - it's a thin wrapper aroung the OpenAI model.\n",
    "\n",
    "In `examples.models`:\n",
    "\n",
    "```python\n",
    "from clip import load as load_clip, tokenize as clip_tokenize\n",
    "\n",
    "class CLIP(torch.nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.model, self.image_preprocess = load_clip(name)\n",
    "\n",
    "    def preprocess(self, r):\n",
    "        if isinstance(r, str):\n",
    "            return clip_tokenize(r, truncate=True)[0, :]\n",
    "        elif isinstance(r, list) and isinstance(r[0], str):\n",
    "            return clip_tokenize(' '.join(r), truncate=True)[0, :]\n",
    "        return self.image_preprocess(r)\n",
    "\n",
    "    def forward(self, r):\n",
    "        if len(r.shape) == 2:\n",
    "            return self.model.encode_text(r)\n",
    "        return self.model.encode_image(r)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0909553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('63f36c946bb7bf3d196c82e6'),\n",
       " 'captions': ['A restaurant has modern wooden tables and chairs.',\n",
       "  'A long restaurant table with rattan rounded back chairs.',\n",
       "  'a long table with a plant on top of it surrounded with wooden chairs ',\n",
       "  'A long table with a flower arrangement in the middle for meetings',\n",
       "  'A table is adorned with wooden chairs with blue accents.'],\n",
       " 'img': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x168>,\n",
       " '_fold': 'train'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee9613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing chunk (1/17)\n",
      "finding documents under filter\n",
      "done.\n",
      "processing with clip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████████████████████████████▋                                                                                              | 1878/5000 [04:48<07:57,  6.54it/s]"
     ]
    }
   ],
   "source": [
    "from examples.models import CLIP\n",
    "\n",
    "docs.create_model(\n",
    "    name='clip',\n",
    "    object=CLIP('RN50'),\n",
    "    filter={},\n",
    "    type='float_tensor',\n",
    "    key='img',\n",
    "    verbose=True,\n",
    "    active=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861db6a0",
   "metadata": {},
   "source": [
    "We'll create a companion model which uses the same underlying object as the previous model. That's specified by adding the name instead of the object in the `object` argument. In this case the model is not `active`, since we'll only be using it for querying the collection. We don't need to specify a `type` since that was done in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.create_model(\n",
    "    name='clip_text',\n",
    "    object='clip',\n",
    "    key='captions',\n",
    "    active=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908e762",
   "metadata": {},
   "source": [
    "We'll also create a measure which tests how similar to each other two outputs might be. Since CLIP was trained with cosine-similarity we'll use that here too.\n",
    "\n",
    "In `examples.measures`:\n",
    "\n",
    "```python\n",
    "\n",
    "def dot(x, y):\n",
    "    return x.matmul(y.T)\n",
    "\n",
    "\n",
    "def css(x, y):\n",
    "    x = x.div(x.norm(dim=1)[:, None])\n",
    "    y = y.div(y.norm(dim=1)[:, None])\n",
    "    return dot(x, y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6cd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.measures import css\n",
    "docs.create_measure('css', css)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8af6c",
   "metadata": {},
   "source": [
    "In order to be able to measure performance on the validation set, we'll add a **metric**.\n",
    "\n",
    "In `examples.metrics`:\n",
    "\n",
    "```python\n",
    "class PatK:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return y in x[:self.k]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66911bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.metrics import PatK\n",
    "docs.create_metric('p_at_10', PatK(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d34b2f",
   "metadata": {},
   "source": [
    "Now we're ready to go to add a **semantic index**. This is a tuple of models, one of which is activated in order to populate the collection with vectors. The idea is that any of the models in the **semantic index** can be used to query the collection using nearest neighbour lookup based on the **measure** chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd71ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from examples.models import CLIP\n",
    "\n",
    "docs.create_semantic_index(\n",
    "    'clip',\n",
    "    models=['clip', 'clip_text'],\n",
    "    measure='css',\n",
    "    metrics=['p_at_10'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416b101",
   "metadata": {},
   "source": [
    "Now the semantic index has been created, we can search through the data using that index.\n",
    "\n",
    "We can see that we can get nice meaningful retrievals using the CLIP model from short descriptive pieces of text.\n",
    "This is very useful, since the model is now deployed to the database, listening for incoming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a38189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson import ObjectId\n",
    "from IPython.display import display\n",
    "\n",
    "docs.semantic_index = 'clip'\n",
    "\n",
    "# example using item id directly\n",
    "for r in docs.find({'$like': {'document': {'_id': ObjectId('63d27372745cc274ef3518f2')}, 'n': 10}}):\n",
    "    display(r['img'])\n",
    "    \n",
    "# or a query which is interpreted by the CLIP model\n",
    "for r in docs.find({'$like': {'document': {'captions': ['Dog catches a frisbee']}, 'n': 5}}):\n",
    "    display(r['img'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76310241",
   "metadata": {},
   "source": [
    "Let's now evaluate the quality of this semantic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.validate_semantic_index('clip', ['text2image_retrieval'], ['p_at_10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f2c87",
   "metadata": {},
   "source": [
    "In the next section of this example, let us train our own model from scratch. The model will be much simpler than the clip model, but will yield faster retrievals. It will be interesting to see how this compares to CLIP, and show-case SuperDuperDB as a framework for easily integrating and benchmarking AI models, in particular for retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3405aee",
   "metadata": {},
   "source": [
    "## Bespoke text-2-image retriever using custom PyTorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9293c",
   "metadata": {},
   "source": [
    "First we will implement a simpler sentence embedding, using a simple word-embedding approach based around Glove.\n",
    "Please look at the model in `examples.models.AverageOfGloves`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://nlp.stanford.edu/data/glove.6B.zip -o data/glove.6B.zip\n",
    "!unzip data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb979e",
   "metadata": {},
   "source": [
    "We may register this model to the collection in the same way we did for the textual part of CLIP. We quote the full code from the `examples` directory here for completeness:\n",
    "\n",
    "In `examples.models`:\n",
    "\n",
    "```python\n",
    "class AverageOfGloves:\n",
    "    def __init__(self, embeddings, index):\n",
    "        self.embeddings = embeddings\n",
    "        self.index = index\n",
    "        self.lookup = dict(zip(self.index, range(len(self.index))))\n",
    "\n",
    "    def preprocess(self, sentence):\n",
    "        if isinstance(sentence, list):\n",
    "            sentence = ' '.join(sentence)\n",
    "        cleaned = re.sub('[^a-z0-9 ]', ' ',  sentence.lower())\n",
    "        cleaned = re.sub('[ ]+', ' ',  cleaned)\n",
    "        words = cleaned.split()\n",
    "        words = [x for x in words if x in self.index]\n",
    "        if not words:\n",
    "            return torch.ones(50).type(torch.float)\n",
    "        ix = list(map(self.lookup.__getitem__, words))\n",
    "        vectors = self.embeddings[ix, :]\n",
    "        return vectors.mean(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625813aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from examples.models import AverageOfGloves\n",
    "\n",
    "with open('data/glove.6B/glove.6B.50d.txt') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "lines = [x.split(' ') for x in lines[:-1]]\n",
    "index = [x[0] for x in lines]\n",
    "vectors = [[float(y) for y in x[1:]] for x in lines]\n",
    "vectors = numpy.array(vectors)\n",
    "\n",
    "glove = AverageOfGloves(torch.from_numpy(vectors).type(torch.float), index)\n",
    "\n",
    "docs.create_model(\n",
    "    'average_glove',\n",
    "    object=glove,\n",
    "    key='captions',\n",
    "    active=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b488f9a",
   "metadata": {},
   "source": [
    "We also need to create a projection layer, which takes the CLIP outputs, and projects them to the same dimensionality as our `average_glove` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d4c37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_model(\n",
    "    'clip_projection',\n",
    "    object=torch.nn.Linear(1024, 50),\n",
    "    active=True,\n",
    "    key='img',\n",
    "    type='float_tensor',\n",
    "    features={'img': 'clip'},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ecddf4",
   "metadata": {},
   "source": [
    "Let's also create a loss function, in order to be able to perform the learning task. Here's the code in full:\n",
    "\n",
    "In `examples.losses`:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "\n",
    "def ranking_loss(x, y):\n",
    "    x = x.div(x.norm(dim=1)[:, None])\n",
    "    y = y.div(y.norm(dim=1)[:, None])\n",
    "    similarities = x.matmul(y.T)\n",
    "    return -torch.nn.functional.log_softmax(similarities, dim=1).diag().mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aebdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.losses import ranking_loss\n",
    "\n",
    "docs.create_loss('ranking_loss', ranking_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a44467",
   "metadata": {},
   "source": [
    "A semantic index training requires:\n",
    "\n",
    "- 1 or more models\n",
    "- A measure function to measure similarity between model outputs\n",
    "- A loss function\n",
    "- One or more validation sets\n",
    "- One or more metrics to measure performance\n",
    "\n",
    "We now have all of these things ready and registered with the database, so we can start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf5276",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_semantic_index(\n",
    "    'simple_image_search',\n",
    "    models=['clip_projection', 'average_glove'],\n",
    "    loss='ranking_loss',\n",
    "    filter={},\n",
    "    projection={'image': 0, '_like': 0},\n",
    "    metrics=['p_at_10'],\n",
    "    measure='css',\n",
    "    validation_sets=['text2image_retrieval'],\n",
    "    batch_size=250,\n",
    "    num_workers=0,\n",
    "    n_epochs=20,\n",
    "    lr=0.001,\n",
    "    log_weights=True,\n",
    "    download=True,\n",
    "    validation_interval=50,\n",
    "    no_improve_then_stop=5,\n",
    "    n_iterations=5000,\n",
    "    use_grads={'clip_projection': True, 'average_glove': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0041c4c",
   "metadata": {},
   "source": [
    "We now can see that we've set and trained our own semantic index. Let's take a look at the results of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6771a45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "info = docs['_semantic_indexes'].find_one({'name': 'simple_image_search'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e0560",
   "metadata": {},
   "source": [
    "We can visualize the improvement of metrics during training using standard functionality from the scientific Python ecosystem. No need for Tensorboards or special visualization interfaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18871e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in info['metric_values']:\n",
    "    if k == 'loss':\n",
    "        print(info['metric_values'][k])\n",
    "        plt.figure()\n",
    "        plt.title('loss')\n",
    "        plt.plot(info['metric_values'][k])\n",
    "        continue\n",
    "    for result in info['metric_values'][k]:\n",
    "        plt.figure()\n",
    "        plt.title(f'{k}/{result}')\n",
    "        plt.plot(info['metric_values'][k][result])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38e56b",
   "metadata": {},
   "source": [
    "The same can be done for the progression of weights during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cf054",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in info['weights']:\n",
    "    plt.figure()\n",
    "    plt.title(parameter)\n",
    "    plt.plot(info['weights'][parameter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e43c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.refresh_model('clip_projection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5e0b6",
   "metadata": {},
   "source": [
    "We see that our model provides quick retrievals using its simpler architecture, and we succeeded in doing this with a very small resource footprint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "docs.semantic_index = 'simple_image_search'\n",
    "for r in docs.find({'$like': {'document': {'captions': ['Dog catches frisbee']}, 'n': 5}}):\n",
    "    display(r['img'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1c62a",
   "metadata": {},
   "source": [
    "Let's see how well our model has done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.validate_semantic_index('simple_image_search', ['text2image_retrieval'], ['p_at_10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb4e49",
   "metadata": {},
   "source": [
    "## Attribute prediction using \"imputations\"; transfer learning, preparation using SpaCy and simple PyTorch linear predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcdfce4",
   "metadata": {},
   "source": [
    "Now we're going to train a different type of model, using a meta-class of machine\n",
    "learning problems we call \"imputation\". The basic idea is to generate tags, by using as training data\n",
    "the noun words extracted from the sentences attached to each image. For this we'll need one model to prepare\n",
    "the data, by extracting these noun words, and another model to predict the tags. Imputations aren't restricted to this however. We can handle any machine learning problem which involves predicting one thing from another thing.\n",
    "This subsumes classification, attribute prediction, generative adversarial learning, language modelling and more.\n",
    "\n",
    "For the noun extraction, we'll use an external library `spacy`. You can see this in the source code `examples.models.NounWords`.\n",
    "\n",
    "In `examples.models`:\n",
    "\n",
    "```python\n",
    "class NounWords:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def preprocess(self, sentences):\n",
    "        sentence = ' '.join(sentences)\n",
    "        nouns = []\n",
    "        for w in self.nlp(sentence):\n",
    "            if w.pos_ == 'NOUN':\n",
    "                nouns.append(str(w).lower())\n",
    "        nouns = sorted(list(set(nouns)))\n",
    "        return nouns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ff23d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from examples.models import NounWords\n",
    "docs.create_model('noun_words', NounWords(), verbose=True, key='captions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f603ea",
   "metadata": {},
   "source": [
    "We'll also need a simple validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aebe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.create_validation_set('attribute_prediction', sample_size=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7cdc72",
   "metadata": {},
   "source": [
    "Let's prepare the tag set we'd like to train on in the next few cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tqdm\n",
    "all_nouns = []\n",
    "for r in tqdm.tqdm(docs.find({'_fold': 'train'}, {'_outputs.captions.noun_words': 1}), \n",
    "                   total=docs.count_documents({})):\n",
    "    all_nouns.extend(r['_outputs']['captions']['noun_words'])\n",
    "    \n",
    "counts = dict(collections.Counter(all_nouns))\n",
    "all_nouns = [w for w in counts if counts[w] > 30]\n",
    "total = docs.count_documents({})\n",
    "pos_weights = [counts[w] / total for w in all_nouns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6032fc",
   "metadata": {},
   "source": [
    "Now we've got our tag set ready, we can create the items necessary to train the model.\n",
    "\n",
    "In `examples.models`:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "\n",
    "class FewHot:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.lookup = dict(zip(tokens, range(len(tokens))))\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        x = [y for y in x if y in self.tokens]\n",
    "        integers = list(map(self.lookup.__getitem__, x))\n",
    "        empty = torch.zeros(len(self.tokens))\n",
    "        empty[integers] = 1\n",
    "        return empty\n",
    "\n",
    "\n",
    "class TopK:\n",
    "    def __init__(self, tokens, n=10):\n",
    "        self.tokens = tokens\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        pred = x.topk(self.n)[1].tolist()\n",
    "        return [self.tokens[i] for i in pred]\n",
    "\n",
    "```\n",
    "\n",
    "In `examples.metrics`:\n",
    "\n",
    "```python\n",
    "def jacquard_index(x, y):\n",
    "    return len(set(x).intersection(set(y))) / len(set(x).union(set(y)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386dc8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.models import FewHot, TopK\n",
    "from examples.metrics import jacquard_index\n",
    "\n",
    "docs.create_model('nouns_to_few_hot', FewHot(all_nouns))\n",
    "docs.create_postprocessor('top_5', TopK(all_nouns, 5))\n",
    "docs.create_forward('attribute_predictor', torch.nn.Linear(1024, len(all_nouns)))\n",
    "docs.create_loss('nouns_loss', torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weights)))\n",
    "docs.create_metric('jacquard_index', jacquard_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43280016",
   "metadata": {},
   "source": [
    "We demonstrate a different way of creating a model. Instead of using a class with `preprocess`, `forward`\n",
    "and `postprocess` methods, we supply these separately. Sometimes this is advantageous when it comes \n",
    "to sharing aspects between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be2dbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_model('attribute_predictor', forward='attribute_predictor', postprocessor='top_5',\n",
    "                  key='img', features={'img': 'clip'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d80959",
   "metadata": {},
   "source": [
    "Let's test the model, using the `apply_model` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06695727",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.apply_model('attribute_predictor', docs.find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea135bb",
   "metadata": {},
   "source": [
    "Now let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e45e59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_imputation(\n",
    "    'noun_prediction',\n",
    "    model='attribute_predictor',\n",
    "    target='nouns_to_few_hot',\n",
    "    loss='nouns_loss',\n",
    "    metrics=['jacquard_index'],\n",
    "    validation_sets=['attribute_prediction'],\n",
    "    lr=0.001,\n",
    "    validation_interval=10,\n",
    "    n_iterations=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30dc496",
   "metadata": {},
   "source": [
    "We can view the results of learning (metrics, loss etc.) by looking in the `_imputations` subcollection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e37bb",
   "metadata": {},
   "source": [
    "## Image captioning using recurrent language modelling and transfer learning based on CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f8993",
   "metadata": {},
   "source": [
    "In the final modelling part of this tutorial, we show that a radically different type of model can be created\n",
    "but which also leverages the `create_imputation` functionality. This is possible, because we utilize a\n",
    "different loss, target, and also use a model which has a different inference `forward` pass than training\n",
    "`forward` pass. This is a very common occurrence, in AI, especially when doing, for example, autoregressive \n",
    "training.\n",
    "\n",
    "The model we create, will be input an image, and will write out a sentence describing that image in unconstrained English. This task is known as \"captioning\" in AI speak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d1e01",
   "metadata": {},
   "source": [
    "This model will leverage a fixed vocabulary of \"allowed\" words. Let us first create this quickly in the \n",
    "following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4166411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "import collections\n",
    "import re\n",
    "\n",
    "all_captions = []\n",
    "n = docs.count_documents({'_fold': 'train'})\n",
    "for r in tqdm.tqdm_notebook(docs.find({'_fold': 'train'}, {'captions': 1, '_id': 0}), total=n):\n",
    "    all_captions.extend(r['captions'])\n",
    "    \n",
    "all_captions = [re.sub('[^a-z ]', '', x.lower()).strip() for x in all_captions]\n",
    "words = ' '.join(all_captions).split(' ')\n",
    "counts = dict(collections.Counter(words))\n",
    "vocab = sorted([w for w in counts if counts[w] > 5 and w])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20f424",
   "metadata": {},
   "source": [
    "Now we can create the model - it utilizes a \"tokenizer\" for preprocessing the captioning data.\n",
    "\n",
    "In `examples.models`:\n",
    "\n",
    "```python\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, tokens, max_length=15):\n",
    "        self.tokens = tokens\n",
    "        if '<unk>' not in tokens:\n",
    "            tokens.append('<unk>')\n",
    "        self._set_tokens = set(self.tokens)\n",
    "        self.lookup = dict(zip(self.tokens, range(len(self.tokens))))\n",
    "        self.dictionary = {k: i for i, k in enumerate(tokens)}\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def preprocess(self, sentence):\n",
    "        sentence = re.sub('[^a-z]]', '', sentence.lower()).strip()\n",
    "        words = [x for x in sentence.split(' ') if x]\n",
    "        words = [x if x in self.tokens else '<unk>' for x in words]\n",
    "        words = words[:self.max_length]\n",
    "        tokenized = list(map(self.lookup.__getitem__, words))\n",
    "        tokenized = tokenized + [len(self) + 1 for _ in range(self.max_length - len(words))]\n",
    "        return torch.tensor(tokenized)\n",
    "\n",
    "\n",
    "class ConditionalLM(torch.nn.Module):\n",
    "    def __init__(self, tokenizer, n_hidden=512, max_length=15, n_condition=1024):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_hidden = n_hidden\n",
    "        self.embedding = torch.nn.Embedding(len(self.tokenizer) + 2, self.n_hidden)\n",
    "        self.conditioning_linear = torch.nn.Linear(n_condition, self.n_hidden)\n",
    "        self.rnn = torch.nn.GRU(self.n_hidden, self.n_hidden, batch_first=True)\n",
    "        self.prediction = torch.nn.Linear(self.n_hidden, len(self.tokenizer) + 2)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def preprocess(self, r):\n",
    "        out = {}\n",
    "        if 'caption' in r:\n",
    "            out['caption'] = [len(self.tokenizer)]  + self.tokenizer.preprocess(r['caption']).tolist()[:-1]\n",
    "        else:\n",
    "            out['caption'] = [len(self.tokenizer)]\n",
    "        out['caption'] = torch.tensor(out['caption'])\n",
    "        if 'img' in r:\n",
    "            out['img'] = r['img']\n",
    "        return out\n",
    "\n",
    "    def train_forward(self, r):\n",
    "        input_ = self.embedding(r['caption'])\n",
    "        img_vectors = self.conditioning_linear(r['img']).unsqueeze(0)\n",
    "        rnn_outputs = self.rnn(input_, img_vectors)[0]\n",
    "        return self.prediction(rnn_outputs)\n",
    "\n",
    "    def forward(self, r):\n",
    "        hidden_states = self.conditioning_linear(r['img']).unsqueeze(0)\n",
    "        predictions = \\\n",
    "            torch.zeros(r['caption'].shape[0], self.max_length).to(r['caption'].device).type(torch.long)\n",
    "        predictions[:, 0] = r['caption'][:, 0]\n",
    "        for i in range(self.max_length - 1):\n",
    "            rnn_outputs, hidden_states = self.rnn(self.embedding(predictions[:, i]).unsqueeze(1),\n",
    "                                                  hidden_states)\n",
    "            logits = self.prediction(rnn_outputs)[:, 0, :]\n",
    "            predictions[:, i + 1] = logits.topk(1, dim=1)[1][:, 0].type(torch.long)\n",
    "        return predictions\n",
    "\n",
    "    def postprocess(self, output):\n",
    "        output = output.tolist()\n",
    "        try:\n",
    "            first_end_token = next(x for x in output if x == len(self.tokenizer) + 2)\n",
    "            output = output[:first_end_token]\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        output = [x for x in output if x < len(self.tokenizer)]\n",
    "        return ' '.join(list(map(self.tokenizer.tokens.__getitem__, output)))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.models import ConditionalLM, SimpleTokenizer\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "m = ConditionalLM(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d9b26",
   "metadata": {},
   "source": [
    "Let us know create the required models necessary for training this model. One of the models is fairly \n",
    "trivial, only used to create the prediction target for the learning task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cd4be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_model('conditional_lm', object=m, active=False, features={'img': 'clip'}, key='_base')\n",
    "docs.create_model('captioning_tokenizer', tokenizer, key='caption', active=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f673e70",
   "metadata": {},
   "source": [
    "We'll use a standard autoregressive loss, of the sort used as a matter of course in language modelling tasks.\n",
    "\n",
    "In `examples.losses`:\n",
    "\n",
    "\n",
    "```python\n",
    "def auto_regressive_loss(x, y):\n",
    "    # start token = x.shape[2] - 2, stop_token = x.shape[2] - 1 (by convention)\n",
    "    stop_token = x.shape[2] - 1\n",
    "    x = x.transpose(2, 1)\n",
    "    losses = torch.nn.functional.cross_entropy(x, y, reduce=False)\n",
    "    not_stops = torch.ones_like(losses)\n",
    "    not_stops[:, 1:] = (y[:, :-1] != stop_token).type(torch.long)\n",
    "    normalizing_factors = not_stops.sum(axis=1).unsqueeze(1)\n",
    "    av_loss_per_row = (losses * not_stops).div(normalizing_factors).sum(axis=1)\n",
    "    return av_loss_per_row.mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e501979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.losses import auto_regressive_loss\n",
    "docs.create_loss('autoregressive_loss', auto_regressive_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe8736",
   "metadata": {},
   "source": [
    "Since each record in the database has several captions per image, we'll need to use a so-called \"splitter\", to \n",
    "align the prediction model and prediction target during training. You can see that the splitter randomly chooses\n",
    "one of the captions to train on for an iteration.\n",
    "\n",
    "In `examples.splitters`:\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "\n",
    "def captioning_splitter(r):\n",
    "    index = random.randrange(len(r['captions']))\n",
    "    target = {}\n",
    "    target['caption'] = r['captions'][index]\n",
    "    r['caption'] = r['captions'][index]\n",
    "    return r, target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.splitters import captioning_splitter\n",
    "\n",
    "docs.create_splitter('captioning_splitter', captioning_splitter)\n",
    "captioning_splitter(docs.find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f82d9",
   "metadata": {},
   "source": [
    "Since we have this new splitter, we need to create a new validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10344946",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_validation_set('captioning', splitter=docs.splitters['captioning_splitter'],\n",
    "                           sample_size=500, chunk_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b8c3c",
   "metadata": {},
   "source": [
    "Now we're ready to start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa5079",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_imputation(\n",
    "    'image_captioner',\n",
    "    model='conditional_lm',\n",
    "    loss='autoregressive_loss',\n",
    "    target='captioning_tokenizer',\n",
    "    splitter='captioning_splitter',\n",
    "    validation_sets=['captioning'],\n",
    "    batch_size=50,\n",
    "    lr=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81fbab",
   "metadata": {},
   "source": [
    "Let's test the model on a sample data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4784f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_docs = list(docs.find().limit(20))\n",
    "images = list(docs.find({}, {'img': 1}).limit(100))\n",
    "\n",
    "results = docs.apply_model('conditional_lm', test_docs, batch_size=10)\n",
    "\n",
    "for r, res in zip(images, results):\n",
    "    display(r['img'])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de1319",
   "metadata": {},
   "source": [
    "Now we have trained and evaluated several models of various types. This includes multiple interacting models with mutual dependencies. In the case of our own efficient semantic search, and also the attribute predictor, these models are downstream of the image clip model, in the sense that at inference time, clip must be present in order to be able to execute these models. In the case of attribute prediction, the training task was downstream from the \n",
    "spacy pipeline for part-of-speech tagging; these tags were used to produce targets for training. However at run-time, the spacy pipeline won't be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74500ec3",
   "metadata": {},
   "source": [
    "The models which we've added and trained are now ready to go, and when new data is added or updated to the collection, they will automatically process this data, and insert the model outputs into the collection documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15fe60",
   "metadata": {},
   "source": [
    "Here is the complete set of models which exist in the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6eedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1fe7c",
   "metadata": {},
   "source": [
    "Not all of these respond to incoming data, for that we need to specify the `active` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f54582",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.list_models(active=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689545a",
   "metadata": {},
   "source": [
    "We can see that these models have processed all documents and their outputs saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59381c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4252fec",
   "metadata": {},
   "source": [
    "## Conclusion and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b01c7",
   "metadata": {},
   "source": [
    "In this tutorial we showed how to deploy interdependent open source, and user supplied PyTorch models on a \n",
    "collection of records, containing image and text. SuperDuperDB can be used on arbitrary data types, and \n",
    "allows users to define their own models. These models can be set up, so that they are interdependent with one another. For example, we applied multiple models in this tutorial which were downstream of the CLIP model. These may additionally be trained or fine-tuned on the \n",
    "data at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
