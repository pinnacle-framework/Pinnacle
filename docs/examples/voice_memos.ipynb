{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae374483",
   "metadata": {},
   "source": [
    "# Cataloguing voice-memos for a self managed personal assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b99fc0",
   "metadata": {},
   "source": [
    "In this example we show-case SuperDuperDB's ability to combine models across data modalities, \n",
    "in this case audio and text, to devise highly sophisticated data based apps, with very little \n",
    "boilerplate code.\n",
    "\n",
    "The aim, is to:\n",
    "\n",
    "- Maintain a database of audio recordings\n",
    "- Index the content of these audio recordings\n",
    "- Search and interrogate the content of these audio recordings\n",
    "\n",
    "We accomplish this by:\n",
    "\n",
    "1. Use a `transformers` model by Facebook's AI team to transcribe the audio to text\n",
    "2. Use an OpenAI vectorization model to index the transcribed text\n",
    "3. Use OpenAI's ChatGPT model in combination with relevant recordings to interrogate the contents\n",
    "  of the audio database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb5a8f4",
   "metadata": {},
   "source": [
    "This functionality could be accomplised using any audio, in particular audio \n",
    "hosted on the web, or in an `s3` bucket. For instance, if you have a repository\n",
    "of audio of conference calls, or memos, this may be indexed in the same way.\n",
    "\n",
    "To make matters simpler, we use a dataset of audio recordings from the `datasets` library, to demonstrate the \n",
    "functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "data = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f71b5",
   "metadata": {},
   "source": [
    "As usual we wrap our MongoDB connector, to connect to the `Datalayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pinnacledb import pinnacle\n",
    "\n",
    "db = pymongo.MongoClient().documents\n",
    "\n",
    "db = pinnacle(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a295099",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.db.client.drop_database('documents')\n",
    "db.db.client.drop_database('_filesystem:documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a8bee",
   "metadata": {},
   "source": [
    "Using an `Encoder`, we may add the audio data directly to a MongoDB collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b288d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinnacledb.db.mongodb.query import Collection\n",
    "from pinnacledb.ext.numpy.array import array\n",
    "from pinnacledb.container.document import Document as D\n",
    "\n",
    "collection = Collection('voice-memos')\n",
    "enc = array('float64', shape=(None,))\n",
    "\n",
    "db.execute(collection.insert_many([\n",
    "    D({'audio': enc(r['audio']['array'])}) for r in data\n",
    "], encoders=(enc,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(collection.find_one()).unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ddd8f",
   "metadata": {},
   "source": [
    "Now that we've done that, let's apply a pretrained `transformers` model to this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222284f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5252c",
   "metadata": {},
   "source": [
    "We wrap this model using the SuperDuperDB wrapper for `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinnacledb.ext.transformers.model import Pipeline\n",
    "\n",
    "transcriber = Pipeline(\n",
    "    identifier='transcription',\n",
    "    object=model,\n",
    "    preprocess=processor,\n",
    "    preprocess_kwargs={'sampling_rate': SAMPLING_RATE, 'return_tensors': 'pt', 'padding': True},\n",
    "    postprocess=lambda x: processor.batch_decode(x, skip_special_tokens=True),\n",
    "    predict_method='generate',\n",
    "    preprocess_type='other',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a14b185",
   "metadata": {},
   "source": [
    "Let's verify this `Pipeline` works on a sample datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ddff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.display.Audio(data[0]['audio']['array'], rate=SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453f913",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transcriber.predict(data[0]['audio']['array'], one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41842efb",
   "metadata": {},
   "source": [
    "Now let's apply the `Pipeline` to all of the audio recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573dccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber.predict(X='audio', db=db, select=collection.find(), max_chunk_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a02b93",
   "metadata": {},
   "source": [
    "We may now verify that all of the recordings have been transcribed in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054796f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(db.execute(\n",
    "    Collection('voice-memos').find({}, {'_outputs.audio.transcription': 1})\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b9220",
   "metadata": {},
   "source": [
    "As in previous examples, we can use OpenAI's text-embedding models to vectorize and search the \n",
    "textual transcriptions directly in MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b27c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinnacledb.container.vector_index import VectorIndex\n",
    "from pinnacledb.container.listener import Listener\n",
    "from pinnacledb.ext.openai.model import OpenAIEmbedding\n",
    "from pinnacledb.db.mongodb.query import Collection\n",
    "\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        identifier='my-index',\n",
    "        indexing_listener=Listener(\n",
    "            model=OpenAIEmbedding(model='text-embedding-ada-002'),\n",
    "            key='_outputs.audio.transcription',\n",
    "            select=Collection(name='voice-memos').find(),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92b56f",
   "metadata": {},
   "source": [
    "Let's confirm this has worked, by searching for the \"royal cavern\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(db.execute(\n",
    "    Collection('voice-memos').like(\n",
    "        {'_outputs.audio.transcription': 'royal cavern'},\n",
    "        n=2,\n",
    "        vector_index='my-index',\n",
    "    ).find({}, {'_outputs.audio.transcription': 1})\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f505a",
   "metadata": {},
   "source": [
    "Now we can connect the previous steps with the `gpt-3.5.turbo`, which is a chat-completion \n",
    "model on OpenAI. The plan is to seed the completions with the most relevant audio recordings, \n",
    "as judged by their textual transcriptions. These transcriptions are retrieved using \n",
    "the previously configured `VectorIndex`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e206af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinnacledb.ext.openai.model import OpenAIChatCompletion\n",
    "\n",
    "chat = OpenAIChatCompletion(\n",
    "    model='gpt-3.5-turbo',\n",
    "    prompt=(\n",
    "        'Use the following facts to answer this question\\n'\n",
    "        '{context}\\n\\n'\n",
    "        'Here\\'s the question:\\n'\n",
    "    ),\n",
    ")\n",
    "\n",
    "db.add(chat)\n",
    "\n",
    "print(db.show('model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb623c4",
   "metadata": {},
   "source": [
    "Let's test the full model! We can ask a question which asks about a specific fact \n",
    "mentioned somewhere in the audio recordings. The model will retrieve the most relevant\n",
    "recordings, and use these in formulating its answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinnacledb.container.document import Document\n",
    "\n",
    "q = 'Is anything really Greek?'\n",
    "\n",
    "print(db.predict(\n",
    "    model='gpt-3.5-turbo',\n",
    "    input=q,\n",
    "    context_select=Collection('voice-memos').like(\n",
    "        Document({'_outputs.audio.transcription': q}), vector_index='my-index'\n",
    "    ).find(),\n",
    "    context_key='_outputs.audio.transcription',\n",
    ")[0].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
