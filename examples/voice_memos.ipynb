{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae374483",
   "metadata": {},
   "source": [
    "# Cataloguing voice-memos for a self managed personal assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4fa500665eccb9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Discover the magic of SuperDuperDB as we seamlessly integrate models across different data modalities, such as audio and text. Experience the creation of highly sophisticated data-based applications with minimal boilerplate code.\n",
    "\n",
    "### Objectives:\n",
    "\n",
    "1. Maintain a database of audio recordings\n",
    "2. Index the content of these audio recordings\n",
    "3. Search and interrogate the content of these audio recordings\n",
    "\n",
    "### Our approach involves:\n",
    "\n",
    "* Utilizing a transformers model by Facebook's AI team to transcribe audio to text.\n",
    "* Employing an OpenAI vectorization model to index the transcribed text.\n",
    "* Harnessing OpenAI ChatGPT model in conjunction with relevant recordings to query the audio database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf9f0ec45cb1f3",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1a857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pinnacledb\n",
    "!pip install transformers soundfile torchaudio librosa openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb5a8f4",
   "metadata": {},
   "source": [
    "Additionally, ensure that you have set your openai API key as an environment variable. You can uncomment the following code and add your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94262bf76c630b10",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    raise Exception('Environment variable \"OPENAI_API_KEY\" not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32971b8afdf76fe5",
   "metadata": {},
   "source": [
    "## Connect to datastore \n",
    "\n",
    "First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the `MongoDB_URI` based on your specific setup. \n",
    "Here are some examples of MongoDB URIs:\n",
    "\n",
    "* For testing (default connection): `mongomock://test`\n",
    "* Local MongoDB instance: `mongodb://localhost:27017`\n",
    "* MongoDB with authentication: `mongodb://pinnacle:pinnacle@mongodb:27017/documents`\n",
    "* MongoDB Atlas: `mongodb+srv://<username>:<password>@<atlas_cluster>/<database>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84da3f2ef58e401",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pinnacledb import pinnacle\n",
    "from pinnacledb.backends.mongodb import Collection\n",
    "import os\n",
    "\n",
    "mongodb_uri = os.getenv(\"MONGODB_URI\",\"mongomock://test\")\n",
    "db = pinnacle(mongodb_uri)\n",
    "\n",
    "# Create a collection for Voice memos\n",
    "voice_collection = Collection('voice-memos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d051e8f5f6f",
   "metadata": {},
   "source": [
    "\n",
    "## Load Dataset\n",
    "\n",
    "In this example se use `LibriSpeech` as our voice recording dataset. It is a corpus of approximately 1000 hours of read English speech. The same functionality could be accomplised using any audio, in particular audio hosted on the web, or in an `s3` bucket. For instance, if you have a repository of audio of conference calls, or memos, this may be indexed in the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab7114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987c9ff85c49fe",
   "metadata": {},
   "source": [
    "## Register Encoders\n",
    "Using an `Encoder`, we may add the audio data directly to a MongoDB collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd038c64e1b4b929",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pinnacledb.ext.numpy import array\n",
    "from pinnacledb import Document\n",
    "\n",
    "enc = array('float64', shape=(None,))\n",
    "\n",
    "db.add(enc)\n",
    "\n",
    "db.execute(voice_collection.insert_many([\n",
    "    Document({'audio': enc(r['audio']['array'])}) for r in data\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f31f4626881e0",
   "metadata": {},
   "source": [
    "## Use a Pre-Trained Model\n",
    "\n",
    "Apply a pretrained `transformers` model to the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222284f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5252c",
   "metadata": {},
   "source": [
    "Wrap the model using the SuperDuperDB wrapper for `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2e38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pinnacledb.ext.transformers import Pipeline\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "transcriber = Pipeline(\n",
    "    identifier='transcription',\n",
    "    object=model,\n",
    "    preprocess=processor,\n",
    "    preprocess_kwargs={'sampling_rate': SAMPLING_RATE, 'return_tensors': 'pt', 'padding': True},\n",
    "    postprocess=lambda x: processor.batch_decode(x, skip_special_tokens=True),\n",
    "    predict_method='generate',\n",
    "    preprocess_type='other',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65b61d45ebdd50",
   "metadata": {},
   "source": [
    "## Run Predictions on a Voice Sample\n",
    "Verify that the `Pipeline` works on a sample datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ddff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# Listen to the sample\n",
    "IPython.display.Audio(data[0]['audio']['array'], rate=SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453f913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the transcript of the sample\n",
    "transcriber.predict(data[0]['audio']['array'], one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83a8b084844292",
   "metadata": {},
   "source": [
    "# Run Predictions on the Whole Recording\n",
    "Apply the `Pipeline` to all audio recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573dccc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcriber.predict(X='audio', db=db, select=voice_collection.find(), max_chunk_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a02b93",
   "metadata": {},
   "source": [
    "Verify that all recordings have been transcribed in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054796f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(db.execute(\n",
    "    voice_collection.find({}, {'_outputs.audio.transcription': 1})\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6cbd8e3d429d9",
   "metadata": {},
   "source": [
    "## Ask Questions to Your Voice Assistant\n",
    "\n",
    "Ask questions to your voice assistant, targeting specific queries and utilizing the power of MongoDB for vector-search and filtering rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedc03c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pinnacledb import VectorIndex, Listener\n",
    "from pinnacledb.ext.openai import OpenAIEmbedding\n",
    "\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        identifier='my-index',\n",
    "        indexing_listener=Listener(\n",
    "            model=OpenAIEmbedding(model='text-embedding-ada-002'),\n",
    "            key='_outputs.audio.transcription',\n",
    "            select=voice_collection.find(),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92b56f",
   "metadata": {},
   "source": [
    "Let's confirm this has worked, by searching for the `royal cavern`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e3e56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the search parameters\n",
    "search_term = 'royal cavern'\n",
    "num_results = 2\n",
    "\n",
    "list(db.execute(\n",
    "    voice_collection.like(\n",
    "        {'_outputs.audio.transcription': search_term},\n",
    "        n=num_results,\n",
    "        vector_index='my-index',\n",
    "    ).find({}, {'_outputs.audio.transcription': 1})\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068514b31268846",
   "metadata": {},
   "source": [
    "## Enrich it with Chat-Completion \n",
    "\n",
    "Connect the previous steps with the gpt-3.5.turbo, a chat-completion model on OpenAI. The plan is to seed the completions with the most relevant audio recordings, as judged by their textual transcriptions. These transcriptions are retrieved using the previously configured `VectorIndex`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e206af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pinnacledb.ext.openai import OpenAIChatCompletion\n",
    "\n",
    "chat = OpenAIChatCompletion(\n",
    "    model='gpt-3.5-turbo',\n",
    "    prompt=(\n",
    "        'Use the following facts to answer this question\\n'\n",
    "        '{context}\\n\\n'\n",
    "        'Here\\'s the question:\\n'\n",
    "    ),\n",
    ")\n",
    "\n",
    "db.add(chat)\n",
    "\n",
    "print(db.show('model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb623c4",
   "metadata": {},
   "source": [
    "## Full Voice-Assistant Experience\n",
    "\n",
    "Test the full model by asking a question about a specific fact mentioned in the audio recordings. The model will retrieve the most relevant recordings and use them to formulate its answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e9da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pinnacledb import Document\n",
    "\n",
    "q = 'Is anything really Greek?'\n",
    "\n",
    "print(db.predict(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    input=q,\n",
    "    context_select=voice_collection.like(\n",
    "        Document({'_outputs.audio.transcription': q}), vector_index='my-index'\n",
    "    ).find(),\n",
    "    context_key='_outputs.audio.transcription',\n",
    ")[0].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
